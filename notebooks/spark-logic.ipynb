{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4777ee4c-fa96-4a4a-af99-168b6183abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e98c8d4-5b6f-45aa-ad2e-f2e5d5ebc92c",
   "metadata": {},
   "source": [
    "### Reading Data From Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5abe71a2-0b88-4056-bed6-f266d1d8fef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/17 21:23:00 WARN Utils: Your hostname, yegane, resolves to a loopback address: 127.0.1.1; using 172.22.201.234 instead (on interface wlp0s20f3)\n",
      "25/11/17 21:23:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/yegane/Documents/smart-meter-simulation/.venv/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/yegane/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/yegane/.ivy2.5.2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6a17117f-b444-464c-8477-387a5b9e8ea2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;4.0.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.9.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.16 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.12.0 in central\n",
      ":: resolution report :: resolve 416ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.12.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.1 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.9.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;4.0.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.2.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.16 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6a17117f-b444-464c-8477-387a5b9e8ea2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/8ms)\n",
      "25/11/17 21:23:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.1\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"smart-meter-data\")\n",
    "    .option(\"startingoffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db83f1-dd92-4a4d-b1e4-b8f6fc9d1558",
   "metadata": {},
   "source": [
    "### Create Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec94ebde-f80e-4826-9907-0c5a8db05ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, TimestampType\n",
    "\n",
    "meter_schema = StructType([\n",
    "    StructField(\"meter_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"building_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"timestamp\", TimestampType(), nullable=False),\n",
    "    StructField(\"power_kw\", FloatType(), nullable=False),\n",
    "    StructField(\"voltage_v\", FloatType(), nullable=False),\n",
    "    StructField(\"status\", IntegerType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644bea9f-93d3-4646-9460-261d2a77a67f",
   "metadata": {},
   "source": [
    "### Parse Raw DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ccf491f-65ae-4753-a2cd-4b1f7074d6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, from_json\n",
    "\n",
    "raw_df = df.selectExpr(\"CAST(value AS STRING)\", \"CAST(key AS STRING)\", \"topic\", \"partition\", \"offset\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70d26258-0635-46a8-8ae6-0032314e99bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df.select(from_json(col(\"value\"), meter_schema).alias(\"data\")).select(\"data.*\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70710b4e-8261-4e40-87d6-212f17ea253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter((df.status >= 0) & (df.status <= 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79444e86-f74b-46f9-ad90-6f6111ce47a5",
   "metadata": {},
   "source": [
    "### Inspect DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a36c151-5f23-42f5-8b3d-ccefec1ba153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# query = df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad701560-7b9b-454f-aa0a-8603030a5401",
   "metadata": {},
   "source": [
    "### House Hourly Power Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216af74a-c843-4315-8dd1-87377ce25f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col, max, min, avg, sum\n",
    "\n",
    "hourly_house_power_consumption = df.groupBy(\n",
    "    \"meter_id\",\n",
    "    window(\"timestamp\", \"1 hour\")\n",
    ").agg(\n",
    "    avg(\"power_kw\").alias(\"avg_power\"),\n",
    "    max(\"power_kw\").alias(\"max_power\"),\n",
    "    min(\"power_kw\").alias(\"min_power\"), \n",
    "    sum(\"power_kw\").alias(\"sum_power\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd50fb1-71b6-423f-85e0-80aee2b709e4",
   "metadata": {},
   "source": [
    "### Building Hourly Power Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1948e8e6-c362-4c2a-b689-c0000cc44634",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_building_power_consumption = df.groupBy(\n",
    "    \"building_id\",\n",
    "    window(\"timestamp\", \"1 hour\")\n",
    ").agg(\n",
    "    avg(\"power_kw\").alias(\"avg_power\"),\n",
    "    max(\"power_kw\").alias(\"max_power\"),\n",
    "    min(\"power_kw\").alias(\"min_power\"),\n",
    "    sum(\"power_kw\").alias(\"sum_power\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe069f-5642-4d9c-957e-852e24380c1f",
   "metadata": {},
   "source": [
    "### Power Consumption Trend - Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1846703-be09-4dec-97bf-25a87f4f1f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col, max, min, avg, sum, count\n",
    "\n",
    "dashboard = (\n",
    "    df\n",
    "    .withWatermark(\"timestamp\", \"5 minutes\")\n",
    "    .groupBy(\n",
    "        window(\"timestamp\", \"1 minute\"),\n",
    "        col(\"meter_id\")\n",
    "    )\n",
    "    .agg(\n",
    "        avg(\"power_kw\").alias(\"avg_power_min\"),\n",
    "        avg(\"voltage_v\").alias(\"avg_voltage_min\"),\n",
    "        count(\"*\").alias(\"event_count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# (\n",
    "#     dashboard.writeStream\n",
    "#     .format(\"console\")  # in production → ClickHouse\n",
    "#     .outputMode(\"complete\")\n",
    "#     .option(\"truncate\", False)\n",
    "#     .start()\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4a9eacf-d13e-4641-8567-a12f799d5357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"SELECT * FROM dashboard_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a63b060-9bbc-40db-a6ee-181c69b7a732",
   "metadata": {},
   "source": [
    "### Power Consumption Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6470ffd5-6a7f-4277-a0e6-d5b7d1980c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly = (\n",
    "    df\n",
    "    .withWatermark(\"timestamp\", \"1 minute\")\n",
    "    .groupBy(\n",
    "        window(\"timestamp\", \"30 seconds\"),\n",
    "        col(\"meter_id\")\n",
    "    ).agg(\n",
    "        avg(\"power_kw\").alias(\"avg_power\"),\n",
    "        max(\"power_kw\").alias(\"max_power\"),\n",
    "        min(\"power_kw\").alias(\"min_power\")\n",
    "    ).withColumn(\n",
    "        \"is_anomaly\",\n",
    "        (col(\"max_power\") > col(\"avg_power\") * 1.8)\n",
    "    )\n",
    ")\n",
    "\n",
    "# (\n",
    "#     anomaly.writeStream\n",
    "#     .format(\"console\")\n",
    "#     .outputMode(\"update\")\n",
    "#     .option(\"truncate\", False)\n",
    "#     .start()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7970d-0622-46c1-9ae6-79185a9a7db1",
   "metadata": {},
   "source": [
    "### Power Consumption - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1be90a-d52d-424e-9987-50a1ec8615fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = (\n",
    "    df\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\")\n",
    "    .groupBy(\n",
    "        window(\"timestamp\", \"5 minutes\"),\n",
    "        col(\"meter_id\")\n",
    "    )\n",
    "    .agg(\n",
    "        avg(\"power_kw\").alias(\"power_5m_avg\"),\n",
    "        stddev(\"power_kw\").alias(\"power_5m_std\"),\n",
    "        avg(\"voltage_v\").alias(\"voltage_5m_avg\"),\n",
    "        stddev(\"voltage_v\").alias(\"voltage_5m_std\"),\n",
    "        max(\"status\").alias(\"max_status\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# (\n",
    "#     features.writeStream\n",
    "#     .format(\"console\")  # in production → write to ClickHouse/Parquet\n",
    "#     .outputMode(\"update\")\n",
    "#     .option(\"truncate\", False)\n",
    "#     .start()\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5383e1d-2472-436e-b191-0a60d7388a00",
   "metadata": {},
   "source": [
    "### Count Animalies / Spikes / Failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd98937d-19aa-4fe9-ae8d-ec408a9e5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_count = (\n",
    "    df\n",
    "    .groupBy(\"meter_id\")\n",
    "    .agg(sum(when(col(\"status\")) == 1, 1).otherwise(0)).alias(\"spike_count\")\n",
    "    .agg(sum(when(col(\"status\")) == 2, 1).otherwise(0)).alias(\"dropout_count\")\n",
    "    .agg(sum(when(col(\"status\")) == 3, 1).otherwise(0)).alias(\"low_consumption_count\")\n",
    "    .agg(sum(when(col(\"status\")) == 1, 1).otherwise(0)).alias(\"voltage_anomaly_count\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3ea479-785b-40e1-9636-3d1460dfd929",
   "metadata": {},
   "source": [
    "### Hourly House Voltage Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b205b49b-370d-42f9-b81e-3d6a9f8d7014",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_house_voltage = house_agg = df.groupBy(\n",
    "    \"meter_id\",\n",
    "    window(\"timestamp\", \"1 hour\")\n",
    ").agg(\n",
    "    avg(\"voltage_v\").alias(\"avg_voltage\"),\n",
    "    max(\"voltage_v\").alias(\"max_voltage\"),\n",
    "    min(\"voltage_v\").alias(\"min_voltage\"), \n",
    "    sum(\"voltage_v\").alias(\"sum_voltage\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8293fba3-6760-4fdf-90fa-a62a5f81f544",
   "metadata": {},
   "source": [
    "### Hourly Building Voltage Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa782a-86d4-4ec9-a158-efa56e45ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_building_voltage = df.groupBy(\n",
    "    \"building_id\",\n",
    "    window(\"timestamp\", \"1 hour\")\n",
    ").agg(\n",
    "    avg(\"voltage_v\").alias(\"avg_voltage\"),\n",
    "    max(\"voltage_v\").alias(\"max_voltage\"),\n",
    "    min(\"voltage_v\").alias(\"min_voltage\"), \n",
    "    sum(\"voltage_v\").alias(\"sum_voltage\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a087f392-2721-4253-b809-a75d49499f14",
   "metadata": {},
   "source": [
    "### Peak Hours / Minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39259080-4a7d-4b8b-8cc7-18a83e4a968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_min_max = (\n",
    "    df.withWatermark(\"timestamp\", \"2 hours\")\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"1 hour\")\n",
    "    ).agg(\n",
    "        max(\"power_kw\").alias(\"max_power_kw\")\n",
    "        min(\"power_kw\").alias(\"min_power_kw\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# max_value = power_min_max.agg(max(\"max_power_kw\")).collect()[0][\"max_power\"]\n",
    "# min_value = power_min_max.agg(min(\"min_power_kw\")).collect()[1][\"min_power\"]\n",
    "\n",
    "peak_hour = df_hourly_max.orderBy(col(\"max_power_kw\").desc()).limit(1)\n",
    "min_hour = df_hourly_max.orderBy(col(\"max_power_kw\").asc()).limit(1)\n",
    "\n",
    "power_min_max.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c716dcf-2eba-45d6-80f9-c80795db28ea",
   "metadata": {},
   "source": [
    "### Write Data To Clickhouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d59c4e-bc1b-43fd-b282-98a0059942c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    df.writeStream.foreachBatch(\n",
    "        lambda batch_df, batch_id: batch_df.write\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", \"jdbc:clickhouse://host:8123/db\")\n",
    "        .option(\"dbtable\", \"iot_max_power\")\n",
    "        .option(\"user\", \"default\")\n",
    "        .option(\"password\", \"\")\n",
    "        .mode(\"append\")\n",
    "        .save()\n",
    "    )\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/iot_clickhouse\")\n",
    "    .start()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv smart-meter)",
   "language": "python",
   "name": "smart-meter-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
