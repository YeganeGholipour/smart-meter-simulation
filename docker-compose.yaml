services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:29092,EXTERNAL://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"

  kowl:
    image: quay.io/cloudhut/kowl:master
    container_name: kowl
    depends_on:
      - kafka
    ports:
      - "8080:8080"
    environment:
      KAFKA_BROKERS: kafka:29092

  spark-master:
    image: apache/spark:4.0.1-scala2.13-java17-python3-r-ubuntu
    container_name: spark-master
    hostname: spark-master
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_NO_DAEMONIZE=true 
    ports:
      - "8081:8080"   # master WEB UI
      - "7077:7077"   # master port / Spark master RPC
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master", "--host", "0.0.0.0"]
    networks:
      - spark-net
  
  spark-worker:
    image: apache/spark:4.0.1-scala2.13-java17-python3-r-ubuntu
    container_name: spark-worker
    hostname: spark-worker
    depends_on:
      - spark-master
    environment:
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=1g
      - SPARK_NO_DAEMONIZE=true
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077", "--host", "0.0.0.0"]
    ports:
      - "8082:8081"
    networks:
      - spark-net

  # spark-client:
  #   image: apache/spark:4.0.1-scala2.13-java17-python3-r-ubuntu
  #   container_name: spark-client
  #   hostname: spark-client
  #   depends_on:
  #     - spark-master
  #   volumes:
  #     - ./app:/opt/app
  #   working_dir: /opt/app
  #   entrypoint: ["/bin/bash", "-c"]
  #   tty: true
  #   networks:
  #     - spark-net

  data_generation:
    container_name: data_generation
    build:
      context: .
      dockerfile: ./services/data_generation/Dockerfile
    env_file:
      - ./services/data_generation/.env
    depends_on:
      - kafka

networks:
  spark-net:
    driver: bridge
  